{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (S.17) Permutation and Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets\n",
    "\n",
    "1. **Set**: *a well-defined colletion of objects*.\n",
    "    - **Math notation:**\n",
    "        - Define a set as $S$ \n",
    "        - If $x$ is an element of set $S$:\n",
    "            - $x \\in S$.\n",
    "        - If $x$ is not an element of set $S$:\n",
    "            - $x\\notin S$.\n",
    "        \n",
    "2. **Subset**: set $T$ is a subset of set $S$ if *every element* in set $T$ is also in set $S$. \n",
    "\n",
    "    - **Math Notation**\n",
    "        - $T$ is a subset of $S$.   \n",
    "            - $T \\subset S$\n",
    "\n",
    "        - $T$ and $S$ can be the SAME population \n",
    "        - If $T$ != $S$, but $T \\subset S$, called a 'proper subset'\n",
    "            - Can use notation:  $T \\subsetneq S$ and $T \\subseteq S$ \n",
    "\n",
    "3. **Universal Set**: The collection of all possible outcomes in a certain context or universe.\n",
    "\n",
    "    - **Math Notation**\n",
    "        - Universal set denoted by Omega ($\\Omega$)\n",
    "        - Can have infinite # of elements (e.g. the set of all real numbers!)\n",
    "        - Example: all possible outcomes of a 6-sided die:\n",
    "            - $\\Omega = \\{1,2,3,4,5,6\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Set Operations**\n",
    "    1. **Union** $S \\cup T$\n",
    "        - all unique elements found in both sets\n",
    "    2. **Intersection** $S \\cap T$.\n",
    "        - All elements of $S$ that also belong to $T$. \n",
    "    3.  **Relative complement / difference** \n",
    "        - all the elements of T that are NOT in S.\n",
    "            - relative complement of S (or $ T\\backslash S $) is $\\{2,4,8\\}$.\n",
    "            - relative complement  of T (or $ S\\backslash T $) is $\\{3,9,12\\}$.\n",
    "    4. **Absolute complement**\n",
    "        - The absolute complement of $S$, with respect to the Universal set $\\Omega$, is the collection of the objects in $\\Omega$ that don't belong to $S$.\n",
    "        - Denoted as $S'$ or $S^c$.\n",
    "        \n",
    "    5. <u> NOTE:</u>Inclusion Exclusion principle\n",
    "        - When combining  2 sets, the method for obtaining the union of two finite sets is given by:\n",
    "\n",
    "            - $\\mid S \\cup T \\mid = \\mid S \\mid + \\mid T \\mid - \\mid S \\cap T \\mid $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Probability\n",
    " 1. **Law of relative frequency**\n",
    "    - Limit of large infinite outcomes produce fixed numbers .\n",
    "        - $$P(E) = \\lim_{n\\to\\infty}\\frac{S(n)}{n}$$\n",
    "    - Probability of Event E having Successful(S) outcomes for $n$ trials\n",
    "    - a given outcome is successful if it is in the event space \n",
    " 2. **Sample Space:**\n",
    "     - All possible outcomes for a given experiment. \n",
    " 3. **Event space:**\n",
    "     - Those outcomes in the sample space that satisfy some critera\n",
    "         - Always a subset of S\n",
    " 4. Probability Axioms\n",
    " \n",
    "    1.  Positivity : Prob is always $0 <= P(E) <=1$\n",
    "\n",
    "    2.  Probability of a certain event: $P(S)=1$\n",
    "\n",
    "    3.  Additivity Union of 2 exclusive sets = sum prob of individual events\n",
    "        happening <br>\n",
    "        - If $A\\cap B = \\emptyset $, then $P(A\\cup B) = P(A) + P(B)$\n",
    "    4. Addition Law:\n",
    "        - Prob of union of A and B is individual P minus intersection\n",
    "$$P(A\\cup B) = P(A) + P(B) - P(A \\cap B)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutations\n",
    "\n",
    "### Permutations of a subset\n",
    "\n",
    "How many ways can we select k elements out of a pool of n objects?\n",
    "- $k$-permutation of $n$:\n",
    "\n",
    "$n*(n-1)*...*(n-k+1)$ or in other words, $P_{k}^{n}= \\dfrac{n!}{(n-k)!}$\n",
    "\n",
    "### Permutations with replacement \n",
    "\n",
    "\\# of possible options doesn‚Äôt change, so n is raised to the power of j, the number of draws from the pop<br><br>\n",
    "$n^j$\n",
    "\n",
    "### Permutations with repetition ...?\n",
    "\n",
    "The \\# of permutations of *n* objects with identical objects of type 1<br>\n",
    "$(n_1^{j_1}* n_2^{j_2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinations\n",
    "- How many ways can we create a subset $k$ out of $n$ objects? \n",
    "    - Unordered\n",
    "$$\\displaystyle\\binom{n}{k} = \\dfrac{P_{k}^{n}}{k!}=\\dfrac{ \\dfrac{n!}{(n-k)!}}{k!} = \\dfrac{n!}{(n-k)!k!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (S.19)  Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "The averages of samples will form a normal distribution,\n",
    "Even if population from which samples are drawn is not normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data); # Draws a histogram and KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.normaltest(data): # Tests if data is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a normal distribution with mean, sd given\n",
    "male_height = scipy.stats.norm(mean, sd)\n",
    "# The result male_height is a SciPy rv object which represents a normal continuous random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use this to generate actual numbers in one go\n",
    "stats.norm.rvs(loc=mean,scale=std_dev,size=number of samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Create numpy arrays of data points that have a distribution of a given rv object</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = rv.mean()\n",
    "std = rv.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numpy to calculate evenly spaced numbers over the specified interval (4 sd) and generate 100 samples.\n",
    "xs = np.linspace(mean - x*std, mean + x*std, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the peak of normal distribution i.e. probability density. \n",
    "ys = rv.pdf(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stats.t.pdf to get values on the probability density function for the t-distribution\n",
    "# the second argument is the degrees of freedom\n",
    "ys = stats.t.pdf(xs, df, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Statistics\n",
    "\n",
    "<ol>\n",
    "\n",
    "<li><b>Sample Distribution:</b></li>  \n",
    "\n",
    ">The distribution of the data points within a single sample.\n",
    "\n",
    "<li><b>Sampl<i><u>ing</u></i> Distribution:</b></li>\n",
    "    \n",
    ">The probability distribution a statistic (for example, the mean of samples) can take\n",
    "\n",
    "<li><b>Sample distribution of the Sample mean: :</b></li>\n",
    "\n",
    ">A distributions of the means of many samples drawn from the population. This forms a normal distribution. \n",
    "\n",
    "<li><b>Sampling Error:</b></li>\n",
    "    \n",
    "> The difference between the sample mean and the population mean.<br><ul><li>This decreases as sample size increases</ul></li>\n",
    "\n",
    "\n",
    "<li><b>Standard error of the sample mean</b></li>  \n",
    "> The std-deviations of the sampling distribution. It is given by\n",
    "\n",
    " > $$ \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <b>Definition</b>\n",
    "    - A range of values\n",
    "    - above and below the point estimate\n",
    "    - that captures the true population parameter\n",
    "    - at some predetermined confidence level.\n",
    "    <br><br>\n",
    "We calculate a confidence interval by taking a point estimate and then adding and subtracting a margin of error to create a range <br><br>\n",
    "    \n",
    "2. <b> Margin of Error</b><br><br>\n",
    "    \n",
    "    1. <u><font size=\"+2\">Pop (œÉ)  </font><b>is known</b></u>:<br>\n",
    "        - use <b> z critical value </b><br><br>\n",
    "            - Definitoin of z - critical value:\n",
    "                - The number of standard deviations you'd have to go \n",
    "                - from the mean of the normal distribution \n",
    "                - to capture the proportion of the data \n",
    "                - associated with the desired confidence level\n",
    "                    - For instance, we know that roughly 95% of the data in a normal distribution lies within 2 standard deviations of the mean, so we could use 2 as the z-critical value for a 95% confidence interval as shown in this image<br><br>\n",
    "            - get z-critical values with:<br><br>\n",
    "                - <mark>stats.norm.ppf(q, loc=0, scale=1)</mark> q = percentile point i.e. 0.95\n",
    "                     - Note that we use stats.norm.ppf(q = 0.975) to get the desired z-critical value instead of q = 0.95 because the distribution has two tails.<br><br>\n",
    "        - <font size=\"+2\">MoE = z ‚àó œÉ / ‚àön</font><br><br>\n",
    "            - <font size=\"+1\">œÉ (sigma)</font> is the population standard deviation\n",
    "            - <u>n</u> is sample size\n",
    "            - <u>z</u> is the z-critical value\n",
    "<br><br>\n",
    "    2. <u><font size=\"+2\">Pop (œÉ)  </font><b>is <u>NOT</u> known</b></u>:<br>\n",
    "        - use a <b> t-critical </b> value\n",
    "            - This is found on a <b>t-distribution</b>, which is like a normal distribution but with heavier tails\n",
    "            - The shape of a t-distribution is determined by a parameter known as <b>degrees of freedom</b>. \n",
    "                - This is esentially the size of the sample\n",
    "                - the higher the dof (i.e. the larger the sample <br> the closer the t-distribution is to a normal distribution <br><br>\n",
    "                \n",
    "        - <font><b>MoE</b> =  $\\bar{x}\\pm t_{\\alpha/2,n-1}\\left(\\dfrac{S}{\\sqrt{n}}\\right)$</font><br><br>\n",
    "            - $\\bar{x}$  is the sample mean\n",
    "            - $t_{\\alpha/2,n-1}$ is the <b> t - critical value</b><br><br>\n",
    "            - $\\dfrac{S}{\\sqrt{n}}$ is the \"standard error of the mean\"(???)\n",
    "            - <u>n</u> is sample size\n",
    "            - <u>$\\alpha$</u> is the confidence interval <br><br>\n",
    "            \n",
    "        - <b>Finding interval with scipy</b>     \n",
    "               \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min and Max of Confidence Interval\n",
    "stats.t.interval(alpha = 0.95,                              # Confidence level\n",
    "                 df= len(sample_chol_levels)-1,             # Degrees of freedom\n",
    "                 loc = x_bar,                               # Sample mean\n",
    "                 scale = s)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                - returns tuple of lower and upper bound of MoE calculated using t-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. <b> Poisson Dist Code</b>\n",
    "    * The Poisson distribution is the discrete probability distribution of the number of events occurring in a given time period, given the average number of times the event occurs over that time period. We shall use a Poisson distribution to construct a bimodal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_ages =stats.poisson.rvs(loc=18, mu=35, size=150000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>QUESTION</b>: \n",
    "\n",
    "1. Can you explain the difference between interpretations more? why cant you say 95% probability <br>\n",
    "2. T-dist: where is this confidence itnerval equation coming from ? $\\dfrac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0,1)$\n",
    "3. what exactly is t_{\\alpha/2,n-1}? is this the t-critical values for two tails? this is what we get from the t-distribution? is this similar to z-critical values? i.e number of deviations from mean on t dist we have to go to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (s.20) Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-vals and null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <b>Null Hypothesis</b>:\n",
    "    - There is no relationship between A and B<br><br>\n",
    "2. <b> Alternative Hypothesis</b>\n",
    "    - The hypothesis traditionally thought of when creating a hypothesis for an experiment  I.E. some effect has occured<br><br>\n",
    "3. **_p-value_**: \n",
    "    - The probability of observing a test statistic at least as large as the one observed, by random chance, assuming that the null hypothesis is true.<br><br>\n",
    "\n",
    "4. $\\alpha$ **_(alpha value)_**: \n",
    "    - The marginal threshold at which you're okay with rejecting the null hypothesis.<br><br>\n",
    "    - <b>If $p < \\alpha$, then you reject the null hypothesis</b> <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect Size\n",
    "\n",
    "<b> Effect Size </b>is used to quantify the size of the difference between two groups under observation <br><br>\n",
    "There are a few ways of making this comparison\n",
    "\n",
    "1. <b><u> Un-standardized or Simple Effect</u></b>\n",
    "\n",
    "    - Look at the **[ difference in means ]** between the two populations\n",
    "        - <b>CAVEATS</b>:\n",
    "            - Without knowing more about the distributions (like the standard deviations or spread of each distribution), it's hard to interpret whether a difference like 15 cm is a big difference or not.\n",
    "            - The magnitude of the difference depends on the units of measure, making it hard to compare across different studies that may be conducted with different units of measurement.\n",
    "            \n",
    "    - There are a number of ways to quantify the difference between distributions. \n",
    "        - A simple option is to express the difference as a percentage of the mean.\n",
    "            - But a problem with relative differences is that you have to choose which mean to express them relative to\n",
    "                - Express as % of pop A or pop B?\n",
    "                \n",
    "                \n",
    "2. <b><u> Overlap Threshold </u></b>\n",
    "    - Choose a threshold between the two means. \n",
    "        - The simple threshold is the midpoint between the means f\n",
    "    - How many members of pop b are below threshold? = <b> b_below </b>\n",
    "    - How many of pop A are above threshold? = <b> a_above </b>\n",
    "    - Calculate overlap (Area under both curves)\n",
    "        - <b> b_below/len(b)   +  a_above/len(a) </b>\n",
    "        - divide by 2 to report fraction of misclassified data points\n",
    "        \n",
    "<br>\n",
    "3. <b><u> Probability of superiority </u></b><br><br>\n",
    "   probability that *\"a randomly-chosen B is greater than a randomly-chosen A\"\n",
    "   \n",
    "   - sum(x > y for x, y in zip(b_sample, a_sample)) / len(b_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. <b><u> Standardized effect size </u></b><br>\n",
    "An effect size statistic that divide effect size by some standardizer i.e. standard deviation:<br>\n",
    "    <b>Effect Size / Standardiser</b><br><br>\n",
    "\n",
    "    1. <b>Cohen's D</b> <br>\n",
    "    One type of standardized effect size statistic.<br>\n",
    "    \n",
    "        -   <u>Formula:</u><br>**$d$ = effect size (difference of means) / pooled standard deviation** <br>\n",
    "            - <b>Pooled standard deviation</b>: Weighted average of the standard deviations of the two groups\n",
    "                -  ( (group1.var * len(group1) + (group2.var * len(group2) )  /  (len(group1) + len(group2)  )\n",
    "                    - This is the variance. Take sqroot to get the pooled std\n",
    "        - <b>Interpretation</b>:<br>\n",
    "        We use these rules of thumb to assess meaning of magnitude of d\n",
    "            - Small effect = 0.2\n",
    "            - Medium Effect = 0.5\n",
    "            - Large Effect = 0.8     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-tests\n",
    "\n",
    "Just as you can use t distribution to provide confidence intervals for estimating the population mean you can also use it to test whether two populations are different, statistically speaking.\n",
    "\n",
    "Use at-test if:\n",
    "    - Don‚Äôt know the population standard deviation \n",
    "    - You have a small sample size\n",
    "\n",
    "\n",
    "\n",
    "1. <b><u> One Sample T-Test</b></u><br>\n",
    "    Used to determine whether<br>\n",
    "    a sample of observations could have been generated<br> \n",
    "    by a process with a specific mean\n",
    "    - For example, you might want to know how your sample mean compares to the population mean\n",
    "        - Does the sample come from the population?(QUESTION)\n",
    "        \n",
    "    - <b>Formula</b> to find t-value of sample \n",
    "    $$t = \\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}}$$\n",
    "    \n",
    "    - To find <b>t - critical value</b>\n",
    "        - scipy.stats.t.ppf(1-alpha, df)\n",
    "            - df is degrees of freedom i.e length of sample\n",
    "    - <u>stats method for conducitng one sample ttest</u>\n",
    "        - scipy.stats.ttest_1samp(a, popmean, axis=0, nan_policy='propagate')\n",
    "            - a = sample mean\n",
    "            -  This function returns the t-value and p-value for the sample\n",
    "    - if t-val is greater than t- crit,\n",
    "        - null hypothesis that sample mean is same as pop mean can be rejected\n",
    "        \n",
    "        \n",
    "\n",
    "2. <b><u> Two Sample T-Test</b></u><br>\n",
    "    used to determine if two population means are equal<br>\n",
    "    Two main types:\n",
    "    - <u>Paired test</u><br>\n",
    "    the individual items/people in the sample will remain the same and researchers are comparing how they change after treatment\n",
    "    - <u>Unpaired(Independant) test</u>\n",
    "    comparing two different, unrelated samples to one another.\n",
    "       \n",
    "        - <mark><a href = \"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\">stats.ttest_ind(experimental, control)</a></mark>\n",
    "            - Calculates the t-test for the means of *two independent* samples of scores.\n",
    "\n",
    "            This is a two-sided test for the null hypothesis that 2 independent samples\n",
    "            have identical average (expected) values. \n",
    "            - returns t statistic and pvalue\n",
    "        - Assumes that the populations the samples have been drawn from have the same variance\n",
    "            '''\n",
    "\n",
    "    <br><br>\n",
    "    - <b> Formulas </b><br>\n",
    "        t statistic:\n",
    "         $$\\large t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{\\sqrt{s^{2}_{p} (\\frac{1}{n_{1}} + \\frac{1}{n_{2}}) }    }  $$ <br>\n",
    "            \n",
    "         Where $s^{2}_{p}$ is the pooled sample variance, calculated as:\n",
    "\n",
    "       $$\\large s^{2}_{p}  = \\frac{(n_{1} -1)s^{2}_{1} +  (n_{2} -1)s^{2}_{2}}{n_{1} + n_{2} - 2}  $$\n",
    "\n",
    "        Where $s^{2}_{1}$ and $s^{2}_{2}$ are the variances for each sample given by the formula \n",
    "        $$ \\large s^{2} = \\frac{\\sum_{i=1}^{n}(x_{i} - \\bar{x})^{2}}{n-1} $$\n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "<br>\n",
    "3. <b><u>Assumptions</b></u>\n",
    "\n",
    "- sample observations have numeric and continuous values\n",
    "- sample observations are independent from each other\n",
    "- the samples have been drawn from normal distributions\n",
    "- if upaired two sample test:\n",
    "    - the populations the samples have been drawn from have the same variance\n",
    "- if paired two-sample test:\n",
    "    - the difference between the two sets of samples are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 1 and Type 2 Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <b><u>Type I error:</u></b>\n",
    "    - rejecting a null hypothesis when it should not have been rejected.\n",
    "        - The confidence level is also the probability that you reject the null hypothesis when it is actually true.\n",
    "    - commonly known as a <b><u>False Positive</u></bb>\n",
    "        \n",
    "2. <b><u>Type II error:</u></b>\n",
    "    \n",
    "    - the probability that you fail to reject the null hypothesis when it is actually false\n",
    "        - aka <u>**False Negatives**</u> or $\\beta$\n",
    "    - related to something called Power_,\n",
    "        - Power = the probability of rejecting the null hypothesis given that it actually is false. \n",
    "            - Power_ = 1 - $\\beta$\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Type1Type2Chart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (S.21) Statistical Power and ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Power\n",
    "\n",
    "- Definition:\n",
    "    - the probability of rejecting the null hypothesis, given that it is indeed false.\n",
    "        - This is a conditional probability: \n",
    "            - P(A) is the probability that you reject the null hypothesis, \n",
    "            - P(B) is the prob that the null is actually false, \n",
    "            - P(A|B) is the prob that y ou reject the null hypothesis given that it is actually false. \n",
    "            \n",
    "- For any given ùõº: \n",
    "    - ùëùùëúùë§ùëíùëü=1‚àíùõΩ.\n",
    "        - ùõº = Type I errors i.e. false positives\n",
    "        - ùõΩ = Type II errors i.e. false negatives\n",
    "\n",
    "- The more power a statistical test has,\n",
    "    - the higher the chance that it will affirm the alternative hypothesis (by rejecting the null hypothesis)\n",
    "    - but also a higher chance that it will lead to more false positives. \n",
    "    \n",
    "- To increase the power of a test we can:\n",
    "    1. increase ùõº:\n",
    "        - This means that we will increase the tolerance for false positives i.e. we may pick samples that have a relatively higher likelihood of actually being from the the distribution of the null hypothesis.\n",
    "    2. increase the sample size (n):\n",
    "        - Ideal, but not always feasible\n",
    "        - makes the distributions for the null and alt hyps narrower,<br>\n",
    "        reducing the areas under each curve \n",
    "    3. increase the effect size:\n",
    "        - i.e. posit a greate difference between the null hypothesis and the alternative hypothesis.\n",
    "\n",
    "<img src=\"./images/PowerDiagram.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how increasing power when the effect size is small is difficult and requires higher sample sizes. Shows how its difficult to detect small changes over the null hypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating and plotting the power of a given independant t-test:\n",
    "\n",
    "from statsmodels.stats.power import TTestIndPower, TTestPower\n",
    "power_analysis = TTestIndPower()\n",
    "power_analysis.plot_power(dep_var='nobs',\n",
    "                          nobs = np.array(range(5,1500)),\n",
    "                          effect_size=np.array([.05, .1, .2,.3,.4,.5]),\n",
    "                          alpha=0.05)\n",
    "\n",
    "# nobs = numbr of observations(goes on x axis)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can also calculate specific values. Simply don't specify one of the four parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate power\n",
    "power_analysis.solve_power(effect_size=.2, nobs1=80, alpha=.05) # power not specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welch's t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ t = \\frac{\\bar{X_1}-\\bar{X_2}}{\\sqrt{\\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}}} = \\frac{\\bar{X_1}-\\bar{X_2}}{\\sqrt{se_1^2+se_2^2}}$\n",
    "\n",
    "where $\\bar{X_i}$ , $s_i$, and $N_i$ are the sample mean, sample variance, and sample size, respectively, for sample i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "$ v \\approx \\frac{\\left( \\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2}\\right)^2}{\\frac{s_1^4}{N_1^2v_1} + \\frac{s_2^4}{N_2^2v_2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "formula = 'S ~ C(E) + C(M) + X'\n",
    "lm = ols(formula, df).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions to Linear Models (S. 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions\n",
    "\n",
    "- two or more variables interact in a non-additive manner\n",
    "    - including them when they're needed will increase your  ùëÖ2  value!\n",
    "- how would you actually include interaction effects in our model? \n",
    "    - To do this, you basically multiply 2 predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "- square your predictor (or raise to 3 or 4 whatever) and \n",
    "- include it in your model as if it were a new predictor. \n",
    "    - so add df[x**2] as a new col to the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn has a built-in polynomial option in the preprocessing module\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "y = yld['Yield']\n",
    "X = yld.drop(columns='Yield', axis=1)\n",
    "\n",
    "poly = PolynomialFeatures(6)\n",
    "X_fin = poly.fit_transform(X)\n",
    "\n",
    "# The transformed feature names are: ['1', 'x0', 'x0^2', 'x0^3', 'x0^4', 'x0^5', 'x0^6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bias Variance Trade off\n",
    "\n",
    "Bias arises when wrong assumptions are made when training a model. For example, an interaction effect is missed, or we didn't catch a certain polynomial relationship. Because of this, our algorithm misses the relevant relations between predictors and the target variable. Note how this is similar to underfitting!\n",
    "\n",
    "Variance arises when a model is too sensitive to small fluctuations in the training set. When variance is high, random noise in the training data is modeled, rather than the intended outputs. This is overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of linear equations\n",
    "\n",
    "The *inverse* of a square matrix *A*, sometimes called a *reciprocal matrix*, is a matrix $A^{-1}$such that\n",
    "\n",
    "> $A \\cdot A^{-1} = I$\n",
    "\n",
    "where $I$ is the identity matrix \n",
    "\n",
    "You need an inverse because you can't perform division operations with matrices! There is no concept of dividing by a matrix. However, you can multiply by an inverse, which achieves the same thing.\n",
    "\n",
    "> $A \\cdot X = B$\n",
    "\n",
    "> $X = A^{-1} \\cdot B$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dot product for these two matrices can be calculated as:\n",
    "\n",
    "import numpy as np\n",
    "A = np.array([[2,1],[3,4]])\n",
    "I = np.array([[1,0],[0,1]])\n",
    "print(I.dot(A))\n",
    "print('\\n', A.dot(I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.linalg.inv(a) #takes in a matrix A and calculates its inverse as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Numpy's built in function solve() to solve linear equations\n",
    "x = np.linalg.solve(A, B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus , Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization ( Ridge and Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "#the Ridge and Lasso models have the parameter alpha, which is Scikit-Learn's version of  ùúÜ  in the regularization cost functions.\n",
    "\n",
    "\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train_transformed, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "\n",
    "In ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\n",
    "\n",
    "$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression \n",
    "Very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term.\n",
    "\n",
    "$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIC\n",
    "\n",
    "$$ \\text{AIC} = -2\\ln(\\hat{L}) + 2k $$\n",
    "\n",
    "Where:\n",
    "* $k$ : length of the parameter space (i.e. the number of features)\n",
    "* $\\hat{L}$ : the maximum value of the likelihood function for the model\n",
    "\n",
    "<b>the model with the lowest AIC should be selected.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIC\n",
    "\n",
    "\\text{BIC} = - 2\\ln(\\hat{L}) + \\ln(n) * k $$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\hat{L}$ and $k$ are the same as in AIC\n",
    "* $n$ : the number of data points (the sample size)\n",
    "\n",
    "<b>Like the AIC, the lower your BIC, the better your model is performing.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummies\n",
    "pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-def485b570ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# liblinear is recommended for small datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Fit the model to the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Instantiate a Logistic regression model\n",
    "# Solver must be specified to avoid warning, see documentation for more information\n",
    "# liblinear is recommended for small datasets\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "regr = LogisticRegression(C=1e5, solver='liblinear')\n",
    "\n",
    "# Fit the model to the training set\n",
    "regr.fit(age, income_bin)\n",
    "\n",
    "#to predict\n",
    "y_hat_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the coefficients\n",
    "coef = regr.coef_\n",
    "interc = regr.intercept_\n",
    "\n",
    "# Create the linear predictor\n",
    "lin_pred = (age * coef + interc)\n",
    "\n",
    "I dont mind waiting an extra month or two if it means I can finally be free of the mind numbing, soul crushing prison that is my current \"career\"?P[[';;p;///[[[[[[[[[p0-;54r/;'# Perform the log transformation\n",
    "mod_income = 1 / (1 + np.exp(-lin_pred))\n",
    "\n",
    "# Sort the numbers to make sure plot looks right\n",
    "age_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Stats Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Create intercept term required for sm.Logit, see documentation for more information\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit model\n",
    "logit_model = sm.Logit(y, X)\n",
    "Hey nick D\n",
    "# Get results of the fit\n",
    "result = logit_model.fit()\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a model's results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(example_labels, example_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalution Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-d00d41ffa443>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d00d41ffa443>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    classification_report() function in the sklearn.metrics\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report() #(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
    "\n",
    "returns d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} $$    \n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} $$  \n",
    "  \n",
    "$$ \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} $$\n",
    "\n",
    "$$ \\text{F1 score} = 2 * \\frac{\\text{Precision * Recall}}{\\text{Precision + Recall}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Scikit-learn's built in roc_curve method returns the fpr, tpr, and thresholds\n",
    "# for various decision boundaries given the case member probabilites\n",
    "\n",
    "# First calculate the probability scores of each of the datapoints:\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC: {}'.format(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Weight\n",
    "\n",
    "- By default the class weights for logistic regression in scikit-learn is None, meaning that both classes will be given equal importance in tuning the model. \n",
    "\n",
    "- Alternatively, you can pass 'balanced' in order to assign weights that are inversely proportional to that class's frequency\n",
    "    - Can pass thes ein as dictionary with key = name of class and value = weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [None, 'balanced', {1:2, 0:1}, {1:10, 0:1}, {1:100, 0:1}, {1:1000, 0:1}]\n",
    "names = ['None', 'Balanced', '2 to 1', '10 to 1', '100 to 1', '1000 to 1']\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for n, weight in enumerate(weights):\n",
    "    # Fit a model\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e20, class_weight=weight, solver='lbfgs')\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    print(model_log)\n",
    "\n",
    "    # Predict\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "    y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over sampling, under sampling, SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- oversampling the minority class or undersampling the majority class can help by producing a synthetic dataset that the learning algorithm is trained on\n",
    "    - important to still maintain a test set from the original(unsamppled) dataset in order to accurately judge the accuracy of the algorithm overall.\n",
    "\n",
    "- Synthetic Minority Oversampling(SMOTE)\n",
    "    - . Here, rather then simply oversampling the minority class with replacement (which simply adds duplicate cases to the dataset), the algorithm generates new sample data by creating 'synthetic' examples that are combinations of the closest minority class cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "ratios = [0.1, 0.25, 0.33, 0.5, 0.7, 1]\n",
    "names = ['0.1', '0.25', '0.33','0.5','0.7','even']\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for n, ratio in enumerate(ratios):\n",
    "    # Fit a model\n",
    "    smote = SMOTE(sampling_strategy=ratio)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train) \n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver ='lbfgs')\n",
    "    model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "    print(model_log)\n",
    "\n",
    "    # Predict\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "    y_score = logreg.decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score, ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhatten Distance\n",
    "\n",
    "$$ \\large d(x,y) = \\sum_{i=1}^{n}|x_i - y_i | $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "$$ \\large d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkwoski Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large d(x,y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^c\\right)^\\frac{1}{c}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K - Nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions, Methods, Objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,  test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "810px",
    "width": "228px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1597px",
    "left": "1715.5px",
    "top": "112px",
    "width": "159px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.4,
   "position": {
    "height": "40px",
    "left": "253px",
    "right": "20px",
    "top": "128px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
